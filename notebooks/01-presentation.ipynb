{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2742801d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What other Topic Models are There?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b0551",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What other Topic Models are There?*\n",
    "___\n",
    "\n",
    "## Contents:\n",
    "\n",
    "1. [What we (often) use](#1.-What-we-(often)-use)\n",
    "    - LDA\n",
    "    - STM\n",
    "2. [Beyond the BOW approach](#2.-Beyond-the-Bag-of-Words-approach)\n",
    "    - CTM\n",
    "3. [Textual Information as Networks](#3.-Textual-Information-as-Networks)\n",
    "    - TopSBM\n",
    "4. [Discussion](#4.-Discussion)\n",
    "5. [Sources](#5.-Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55568a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. What we (often) use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600dbb9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use*\n",
    "___\n",
    "\n",
    "## A very short introduction to LDA and STM\n",
    "\n",
    "- basic idea: use text as data and try to understand what a text is about\n",
    "- three main components and a \"target\": words, documents, corpora and *topics*\n",
    "- closely related to dimensionality reduction\n",
    "    - tf-idf\n",
    "    - LSI/pLSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b794cb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use - LDA and STM*\n",
    "___\n",
    "- LDA [[1](#Sources)] assumes a set of underlying topics for a corpus of documents and a distribution of all words over those topics\n",
    "\n",
    "\n",
    "- this way we get \n",
    "    - probabilities for documents to belong to certain topics\n",
    "    - a characterization of topics by frequent words\n",
    "    - information about the topic proportions in our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d271b8ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use - LDA and STM*\n",
    "___\n",
    "<img src=\"../images/blei_tm.png\" width=\"1100\" height=\"1100\">\n",
    "\n",
    "___\n",
    "[[2](#5.-Sources)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47429a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use - LDA and STM*\n",
    "___\n",
    "- STM [[3](#Sources)] extends correlated topic models (also abbreviated by CTM) which in turn improved LDA\n",
    "    - introduction of a linear term for topic probabilities\n",
    "    - covariates (e.g. publication date and/or source) can be used to to get a better representation of topic prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a1470c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use - LDA and STM*\n",
    "___\n",
    "LDA                        |  STM\n",
    ":-------------------------:|:-------------------------:\n",
    "<img src=\"../images/lda_full.png\" width=\"1200\" height=\"1200\">   |  <img src=\"../images/stm_full.png\" width=\"1200\" height=\"1200\">\n",
    "\n",
    "___\n",
    "Own images after Blei et al. 2003 [[1](#5.-Sources)] and Stewart et al. 2013 [[3](#5.-Sources)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671fa38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use - LDA and STM*\n",
    "___\n",
    "## Pros and Cons\n",
    "\n",
    "- LDA is widely applied and can be used in R and Python\n",
    "- does not allow covariates\n",
    "\n",
    "\n",
    "- STM is only implemented in R\n",
    "- covariates (supposedly) make the model more interpretable\n",
    "- not as widely used as LDA (yet)\n",
    "\n",
    "\n",
    "- both rely on the BOW approach\n",
    "- both are questionable for short documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0109ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 2. Beyond the Bag of Words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920fdfb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Beyond the BOW approach*\n",
    "___\n",
    "## Contextualized Topic Modeling (CTM):\n",
    "- CTM [[4](#Sources)] uses pre-trained language models to overcome the BOW approach by using semantic and syntactic context information\n",
    "- The main point of interest: pre-trained language models specifically, **BERT** [[5](#Sources)] (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8bbef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Beyond the BOW approach - CTM*\n",
    "___\n",
    "- Transformers are deep learning algorithms that can predict outcomes* from contextual information\n",
    "- used e.g. for translation tasks\n",
    "- computationally expensive to train but relatively cheap to implement once trained\n",
    "- competitive or even state of the art performance in top language modelling tasks\n",
    "- no one really knows why\n",
    "\n",
    "___\n",
    "\\* E.g.: what is the next sentence *y* if we have sentence *x* before and sentence *z* after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be15ecfe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36041621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextualized_topic_models.models.ctm import CombinedTM\n",
    "from contextualized_topic_models.utils.data_preparation import TopicModelDataPreparation\n",
    "from contextualized_topic_models.utils.data_preparation import bert_embeddings_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f39d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "qt = TopicModelDataPreparation(\"paraphrase-distilroberta-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6257dbf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "path_data = '../data/'\n",
    "\n",
    "fname_data = 'corpus.txt'\n",
    "filename = os.path.join(path_data, fname_data)\n",
    "\n",
    "with open(filename,'r', encoding = 'utf8') as f:\n",
    "    x = f.readlines()\n",
    "texts = [h.split() for h in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2083c51b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be3b1b437e61448c9f7f78ab051ebc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/736 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b87b81616545ba8b4d129cf0b44afb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dce70773a5745fd947d5f482ee932e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/686 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9ac566c42f94f569fbb96b69b74fa5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3460dced786c4aa09601ab02f812251e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "750de28ca43641c0b7e251149a1d0b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863c5d961f7e4868892f1fff51bf9533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205c3a7d1add4c85a64596c22fb3ce20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cfd05b6867044d0b9c631ca76e6c6fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a55b36b2c64a14b389a62ac13c2adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a17a8186654f408aa2724b66e93bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a23d6ae55e84a27a4f2c01a4918143f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b53b5aed22b4dceb97dcc7757ee8bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce143ddd96e04560b2567d40de0db716",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ungers/anaconda3/envs/tm_alternatives/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "training_dataset = qt.fit(text_for_contextual=x, text_for_bow=x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "804fd728",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "ctm = CombinedTM(bow_size=len(qt.vocab), contextual_size=768, n_components=21) # 50 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "606c356e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [100/100]\t Seen Samples: [6300/6300]\tTrain Loss: 4462.190972222223\tTime: 0:00:14.718173: : 100it [26:50, 16.11s/it]\n",
      "Sampling: [20/20]: : 20it [04:47, 14.37s/it]\n"
     ]
    }
   ],
   "source": [
    "ctm.fit(training_dataset) # run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8918e389",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {0: ['targets',\n",
       "              'june',\n",
       "              'catalytic',\n",
       "              'passing',\n",
       "              'top',\n",
       "              'knowledge',\n",
       "              'production',\n",
       "              'extent',\n",
       "              'performing',\n",
       "              'authors'],\n",
       "             1: ['atoms',\n",
       "              'assessment',\n",
       "              'concentrations',\n",
       "              'informatics',\n",
       "              'evolutionary',\n",
       "              'complete',\n",
       "              'operations',\n",
       "              'circle',\n",
       "              'dynamical',\n",
       "              'infection'],\n",
       "             2: ['diffraction',\n",
       "              'right',\n",
       "              'polarizability',\n",
       "              'alpha',\n",
       "              'measured',\n",
       "              'refraction',\n",
       "              'real',\n",
       "              'frequency',\n",
       "              'spacing',\n",
       "              'requires'],\n",
       "             3: ['and',\n",
       "              'of',\n",
       "              'the',\n",
       "              'in',\n",
       "              'for',\n",
       "              'that',\n",
       "              'as',\n",
       "              'to',\n",
       "              'with',\n",
       "              'is'],\n",
       "             4: ['obtained',\n",
       "              'stages',\n",
       "              'confirm',\n",
       "              'efficiently',\n",
       "              'employ',\n",
       "              'began',\n",
       "              'settings',\n",
       "              'pathogen',\n",
       "              'consensus',\n",
       "              'downward'],\n",
       "             5: ['laws',\n",
       "              'body',\n",
       "              'frame',\n",
       "              'public',\n",
       "              'forces',\n",
       "              'since',\n",
       "              'law',\n",
       "              'science',\n",
       "              'if',\n",
       "              'ignorance'],\n",
       "             6: ['where',\n",
       "              'two',\n",
       "              'energy',\n",
       "              'given',\n",
       "              'there',\n",
       "              'electron',\n",
       "              'electric',\n",
       "              'because',\n",
       "              'detector',\n",
       "              'product'],\n",
       "             7: ['confused',\n",
       "              'underneath',\n",
       "              'operations',\n",
       "              'list',\n",
       "              'accommodate',\n",
       "              'metabolism',\n",
       "              'explanation',\n",
       "              'whether',\n",
       "              'targets',\n",
       "              'sent'],\n",
       "             8: ['since',\n",
       "              'an',\n",
       "              'where',\n",
       "              'laws',\n",
       "              'there',\n",
       "              'no',\n",
       "              'information',\n",
       "              'two',\n",
       "              'mass',\n",
       "              'force'],\n",
       "             9: ['fast',\n",
       "              'referred',\n",
       "              'car',\n",
       "              'pi',\n",
       "              'evaluate',\n",
       "              'wall',\n",
       "              'originally',\n",
       "              'physicist',\n",
       "              'arrays',\n",
       "              'maximal'],\n",
       "             10: ['referred',\n",
       "              'section',\n",
       "              'shown',\n",
       "              'results',\n",
       "              'assessment',\n",
       "              'operations',\n",
       "              'fast',\n",
       "              'measured',\n",
       "              'order',\n",
       "              'accelerator'],\n",
       "             11: ['cpus',\n",
       "              'successor',\n",
       "              'remarkable',\n",
       "              'list',\n",
       "              'operations',\n",
       "              'pi',\n",
       "              'displaces',\n",
       "              'photoabsorption',\n",
       "              'carbon',\n",
       "              'planes'],\n",
       "             12: ['emerging',\n",
       "              'oxygen',\n",
       "              'mutation',\n",
       "              'accurately',\n",
       "              'central',\n",
       "              'address',\n",
       "              'little',\n",
       "              'times',\n",
       "              'track',\n",
       "              'reflecting'],\n",
       "             13: ['by',\n",
       "              'the',\n",
       "              'are',\n",
       "              'with',\n",
       "              'an',\n",
       "              'or',\n",
       "              'can',\n",
       "              'of',\n",
       "              'protein',\n",
       "              'be'],\n",
       "             14: ['are',\n",
       "              'by',\n",
       "              'that',\n",
       "              'of',\n",
       "              'is',\n",
       "              'and',\n",
       "              'in',\n",
       "              'the',\n",
       "              'or',\n",
       "              'an'],\n",
       "             15: ['by',\n",
       "              'are',\n",
       "              'with',\n",
       "              'an',\n",
       "              'in',\n",
       "              'it',\n",
       "              'be',\n",
       "              'for',\n",
       "              'the',\n",
       "              'or'],\n",
       "             16: ['list',\n",
       "              'driven',\n",
       "              'displaces',\n",
       "              'simultaneously',\n",
       "              'aligned',\n",
       "              'address',\n",
       "              'highways',\n",
       "              'primer',\n",
       "              'homologous',\n",
       "              'uniform'],\n",
       "             17: ['or',\n",
       "              'by',\n",
       "              'data',\n",
       "              'for',\n",
       "              'as',\n",
       "              'such',\n",
       "              'in',\n",
       "              'at',\n",
       "              'that',\n",
       "              'to'],\n",
       "             18: ['remained',\n",
       "              'originally',\n",
       "              'publications',\n",
       "              'production',\n",
       "              'problematic',\n",
       "              'observe',\n",
       "              'limits',\n",
       "              'top',\n",
       "              'sufficient',\n",
       "              'publicly'],\n",
       "             19: ['new',\n",
       "              'assessment',\n",
       "              'science',\n",
       "              'list',\n",
       "              'web',\n",
       "              'scientists',\n",
       "              'general',\n",
       "              'informatics',\n",
       "              'tools',\n",
       "              'drawing'],\n",
       "             20: ['metabolism',\n",
       "              'emerging',\n",
       "              'problematic',\n",
       "              'organism',\n",
       "              'eccb',\n",
       "              'sequenced',\n",
       "              'accommodate',\n",
       "              'circle',\n",
       "              'tertiary',\n",
       "              'whether']})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctm.get_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c686a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Beyond the BOW approach - CTM*\n",
    "___\n",
    "### why should we care about CTM?\n",
    "- context leads to an increase in coherence compared to LDA \n",
    "- can use pre-trained models for different domains and languages\n",
    "- multi-language topic modeling [[6](#Sources)]\n",
    "- there are already implementations (at least for Python) [[7](#Sources)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918c1c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 3. Textual Information as Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec892",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Textual Information as Networks*\n",
    "___\n",
    "## hSBM - Topic models based on Stochastic Block Models\n",
    "\n",
    "- Block Modeling is a method of community detection used in social network analysis (SNA) [[8](#Sources)]\n",
    "- the used network structure is a (weighted) bipartite network based on the word-document matrix*\n",
    "- Hierarchical stochastic block modeling [[9](#Sources)] is implemented to mirror LDA in the network approach \n",
    "\n",
    "___\n",
    "\\* Words and documents are nodes that are connected if a word occurs within a document. This way, words can be linked via documents and vice versa. The word frequency is reflected in weighted ties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9db0a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Textual Information as Networks - hSBM*\n",
    "___\n",
    "<img src=\"../images/gerlach_tm.png\" width=\"700\" height=\"700\">\n",
    "\n",
    "[[9](#5.-Sources)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33f943d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Textual Information as Networks - hSBM*\n",
    "___\n",
    "### Why should we care about hSBM?\n",
    "\n",
    "- the model is more agnostic towards the topic distribution and therefore more appropriate to address known properties of textual data such as Zipf's Law\n",
    "- outperforms LDA in minimum description length* in most settings \n",
    "- there is an implementation that is sort of ready in Python: TopSBM [[10](#Sources), [11](#Sources)]\n",
    "- The number of topics can be inferredfrom the model\n",
    "- Combines NLP and SNA\n",
    "\n",
    "___\n",
    "\\* Measures how parsimonious a model is in describing the data, lower is better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a742405",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 4. Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2414209",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Discussion*\n",
    "___\n",
    "- Why do we (maybe) still use LDA and STM?\n",
    "- What is missing?\n",
    "- Should we use different models in the future?\n",
    "- How should models be compared and validated (for our usecases)\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67967281",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Sources\n",
    "\n",
    "- [[1]](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=https://githubhelp.com) Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022.\n",
    "\n",
    "\n",
    "- [[2]](https://dl.acm.org/doi/10.1145/2133806.2133826) Blei, David M. \"Probabilistic topic models.\" Communications of the ACM 55.4 (2012): 77-84.\n",
    "\n",
    "\n",
    "- [[3]](https://www.jstatsoft.org/article/view/v091i02) Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. \"Stm: An R package for structural topic models.\" Journal of Statistical Software 91 (2019): 1-40.\n",
    "\n",
    "\n",
    "- [[4]](https://arxiv.org/abs/2004.03974) Bianchi, Federico, Silvia Terragni, and Dirk Hovy. \"Pre-training is a hot topic: Contextualized document embeddings improve topic coherence.\" arXiv preprint arXiv:2004.03974 (2020).\n",
    "\n",
    "\n",
    "- [[5]](https://arxiv.org/abs/1810.04805v2) Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a6a42",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- [[6]](https://arxiv.org/abs/2004.07737) Bianchi, Federico, et al. \"Cross-lingual contextualized topic models with zero-shot learning.\" arXiv preprint arXiv:2004.07737 (2020).\n",
    "\n",
    "\n",
    "- [[7]](https://github.com/MilaNLProc/contextualized-topic-models) Contextualized Topic Modeling on github.\n",
    "\n",
    "\n",
    "- [[8]](https://methods.sagepub.com/book/the-sage-handbook-of-social-network-analysis/n31.xml) Van Duijn, Marijtje AJ, and Mark Huisman. \"Statistical models for ties and actors.\" The SAGE handbook of social network analysis (2011): 459-483.\n",
    "\n",
    "\n",
    "- [[9]](https://www.science.org/doi/10.1126/sciadv.aaq1360) Gerlach, Martin, Tiago P. Peixoto, and Eduardo G. Altmann. \"A network approach to topic models.\" Science advances 4.7 (2018): eaaq1360.\n",
    "\n",
    "\n",
    "- [[10]](https://topsbm.github.io/) Topic Models based on Stochastic Block Models Blog on github\n",
    "\n",
    "\n",
    "- [[11]](https://github.com/martingerlach/hSBM_Topicmodel) hSBM Topic Model on github"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "theme": "moon"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
