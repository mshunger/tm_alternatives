{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2742801d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What other Topic Models are There?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b0551",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What other Topic Models are There?*\n",
    "___\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- [What we (often) use](#What-we-(often)-use)\n",
    "    - LDA\n",
    "    - STM\n",
    "- [Beyond the BOW approach](#Beyond-the-Bag-of-Words-approach)\n",
    "    - CTM\n",
    "- [Textual Information as Networks](#Textual-Information-as-Networks)\n",
    "    - TopSBM\n",
    "- [Sources](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55568a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What we (often) use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c600dbb9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use*\n",
    "___\n",
    "\n",
    "## A very short introduction to LDA and STM\n",
    "\n",
    "- basic idea: use text as data and try to understand what a text is about\n",
    "- three main components and a \"target\": words, documents, corpora and *topics*\n",
    "- closely related to dimensionality reduction\n",
    "    - tf-idf\n",
    "    - LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b794cb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use - LDA and STM*\n",
    "___\n",
    "- LDA [[1]](#Sources) assumes a set of underlying topics for a corpus of documents and a distribution of all words over those topics\n",
    "\n",
    "\n",
    "- this way we get \n",
    "    - probabilities for documents to belong to certain topics\n",
    "    - a characterization of topics by frequent words\n",
    "    - information about the topic proportions in our corpus\n",
    "\n",
    "\n",
    "- STM [[2]](#Sources) extends LDA\n",
    "    - introduction of a linear term for topic probabilities\n",
    "    - covariates (e.g. publication date and/or source) can be used to to get a better representation of topic prevalence\n",
    "    \n",
    "- [ ] Can an STM be reduced to an LDA?\n",
    "- [ ] Does STM also have a dirichlet prior?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f671fa38",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*What we (often) use - LDA and STM*\n",
    "___\n",
    "## Pros and Cons\n",
    "\n",
    "- LDA is widely applied and can be used in R and Python\n",
    "- does not allow covariates\n",
    "\n",
    "\n",
    "- STM is only implemented in R\n",
    "- covariates (supposedly) make the model more interpretable\n",
    "- not as widely used as LDA (yet)\n",
    "\n",
    "\n",
    "- both rely on the BOW approach\n",
    "- both are questionable for short documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0109ef",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond the Bag of Words approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c920fdfb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Beyond the BOW approach*\n",
    "___\n",
    "## Contextualized Topic Modeling (CTM):\n",
    "- CTM [[3]](#Sources) uses pre-trained language models to overcome the BOW approach by using semantic and syntactic context information\n",
    "- The main point of interest: pre-trained language models specifically, **BERT** [[4]](#Sources) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8bbef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Beyond the BOW approach - CTM*\n",
    "___\n",
    "- Transformers are deep learning algorithms that can predict outcomes* from contextual information\n",
    "- used e.g. for translation tasks\n",
    "- computationally expensive to train but relatively cheap to implement once trained\n",
    "- competitive if not even state of the art performance in top language modelling tasks\n",
    "- no one really knows why\n",
    "\n",
    "___\n",
    "\\* E.g.: what is the next sentence *y* if we have sentence *x* before and sentence *z* after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217c686a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Beyond the BOW approach - CTM*\n",
    "___\n",
    "### why should we care about CTM?\n",
    "- context leads to an increase in coherence compared to LDA \n",
    "- can use pre-trained models for different domains and languages\n",
    "- multi-language topic modeling [[5]](#Sources)\n",
    "- there are already implementations (at least for Python) [[6]](#Sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0918c1c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Textual Information as Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec892",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Textual Information as Networks*\n",
    "___\n",
    "## TopSBM - Topic models based on Stochastic Block Models\n",
    "\n",
    "- Block Modeling is a method of community detection used in social network analysis (SNA) [[7]](#Sources)\n",
    "- the used network structure is a (weighted) bipartite network based on the word-document matrix*\n",
    "\n",
    "___\n",
    "\\* Words and documents are nodes that are connected if a word occurs within a document. This way, words can be linked via documents and vice versa. The word frequency is reflected in weighted ties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a6a42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sources\n",
    "\n",
    "[[1]](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf?ref=https://githubhelp.com) Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022.\n",
    "\n",
    "\n",
    "- [[2]](https://www.jstatsoft.org/article/view/v091i02) Roberts, Margaret E., Brandon M. Stewart, and Dustin Tingley. \"Stm: An R package for structural topic models.\" Journal of Statistical Software 91 (2019): 1-40.\n",
    "\n",
    "\n",
    "- [[3]](https://arxiv.org/abs/2004.03974) Bianchi, Federico, Silvia Terragni, and Dirk Hovy. \"Pre-training is a hot topic: Contextualized document embeddings improve topic coherence.\" arXiv preprint arXiv:2004.03974 (2020).\n",
    "\n",
    "\n",
    "- [[4]](https://arxiv.org/abs/1810.04805v2) Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).\n",
    "\n",
    "\n",
    "- [[5]](https://arxiv.org/abs/2004.07737) Bianchi, Federico, et al. \"Cross-lingual contextualized topic models with zero-shot learning.\" arXiv preprint arXiv:2004.07737 (2020).\n",
    "\n",
    "\n",
    "- [[6]](https://github.com/MilaNLProc/contextualized-topic-models) Contextualized Topic Modeling on github.\n",
    "\n",
    "\n",
    "- [[7]](https://methods.sagepub.com/book/the-sage-handbook-of-social-network-analysis/n31.xml) Van Duijn, Marijtje AJ, and Mark Huisman. \"Statistical models for ties and actors.\" The SAGE handbook of social network analysis (2011): 459-483.\n",
    "\n",
    "- [[]](https://www.science.org/doi/10.1126/sciadv.aaq1360) Gerlach, Martin, Tiago P. Peixoto, and Eduardo G. Altmann. \"A network approach to topic models.\" Science advances 4.7 (2018): eaaq1360.\n",
    "\n",
    "\n",
    "- [[]](https://topsbm.github.io/) Topic Models based on Stochastic Block Models Blog on github\n",
    "\n",
    "\n",
    "- [[]](https://github.com/martingerlach/hSBM_Topicmodel) hSBM Topic Model on github"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6b39a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overall to dos:\n",
    "\n",
    "- [ ] Examples and images\n",
    "- [ ] 'Hands on' examples with code\n",
    "- [ ] Open Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918e389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
